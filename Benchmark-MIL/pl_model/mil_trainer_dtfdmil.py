# pl_model/mil_trainer_dtfdmil.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
import random
from torchmetrics import MetricCollection
from torchmetrics.classification import MulticlassAccuracy, MulticlassCalibrationError

from pl_model.optimizers import Lookahead, Lion
from pl_model.forward_fn import dtfdmil_forward_1st_tier, dtfdmil_forward_2nd_tier


class DTFDTrainerModule(pl.LightningModule):
    def __init__(
        self,
        args,
        seed,
        test_dataset_element_name,
        test_class_names_list,
        resolution_str,
        classifier_list,
        loss_func_list,
        metrics,
        patch_path=None,
    ):
        super(DTFDTrainerModule, self).__init__()
        self.save_hyperparameters(ignore=["classifier_list", "loss_func_list", "metrics"])

        self.args = args
        self.seed = seed
        self.test_dataset_element_name = test_dataset_element_name
        self.test_class_names_list = test_class_names_list
        self.patch_path = patch_path

        # ğŸ”¥ main.pyì—ì„œ ë§Œë“  ê³µí†µ ê²½ë¡œ (results/TCGA-RCC/x10/256/DTFD-MIL-AFS/hibou_b/)
        self.base_save_dir = getattr(self.args, "base_save_dir", None)

        # data1 ì¼€ì´ìŠ¤: colon 7/8-class ë¼ë²¨ ì´ë¦„ ê³ ì • (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        if "data1" in self.test_dataset_element_name:
            if getattr(self.args, "label_type", "") in ["label1_8_class", "label2_8_probs"]:
                self.test_class_names_list = [
                    "HP", "IP", "LP", "SSL", "TA", "TSA", "TVA+VA", "Other"
                ]
            else:
                self.test_class_names_list = ["HP", "IP", "LP", "SSL", "TA", "TSA", "TVA+VA"]

        self.num_classes = len(self.test_class_names_list)
        self.resolution_str = resolution_str

        # DTFD-MILì€ Manual Optimization í•„ìˆ˜
        self.automatic_optimization = False

        # classifier_list êµ¬ì¡° ë¶„í•´
        self.classifier = classifier_list[0]
        self.attention = classifier_list[1]
        self.dimReduction = classifier_list[2]
        self.UClassifier = classifier_list[3]

        # Loss í•¨ìˆ˜ (Tier-1, Tier-2)
        self.loss_func0 = loss_func_list[0]  # 1st-tier loss
        self.loss_func1 = loss_func_list[1]  # 2nd-tier loss (slide-level)

        self.accumulate_grad_batches = args.accumulate_grad_batches

        # Metrics ì„¤ì • (TorchMetrics í™œìš©)
        balanced_acc = MulticlassAccuracy(num_classes=self.num_classes, average="macro")

        self.train_metrics = metrics.clone(prefix="train/")
        self.val_metrics = MetricCollection(
            {
                "acc": metrics.clone(),
                "balanced_acc": balanced_acc.clone(),
            },
            prefix="val/",
        )

        self.test_metrics = MetricCollection(
            {
                "acc": metrics.clone(),
                "balanced_acc": balanced_acc.clone(),
                "ece": MulticlassCalibrationError(
                    num_classes=self.num_classes,
                    n_bins=args.n_bins,
                ),
            },
            prefix="final_test/",
        )

        # Callbackìœ¼ë¡œ ë„˜ê²¨ì¤„ ë°ì´í„° ë²„í¼
        self.test_outputs = []

        # ğŸ”¥ save_metrics.py í˜¸í™˜ìš© ë²„í¼ (seedë³„ / ensemble ê³„ì‚°ìš©)
        self.y_prob_list = []
        self.label_list = []
        self.names = []
        self.logits = None
        self.labels = None

        # Patch Drop ì„¤ì •
        self.mil_patch_drop_min = float(getattr(self.args, "mil_patch_drop_min", 0.0))
        self.mil_patch_drop_max = float(getattr(self.args, "mil_patch_drop_max", 0.0))
        self._use_patch_drop = self.mil_patch_drop_max > 0.0

        self.use_weighted_sampler = bool(getattr(self.args, "use_weighted_sampler", False))

        # ğŸ”¥ attention ì‚¬ìš© ì—¬ë¶€ flag
        self.attention_func = bool(getattr(self.args, "attention", False))

    # ------------------------------------------------------------------
    # bag ë‚´ patch ëœë¤ ì„œë¸Œìƒ˜í”Œë§ í•¨ìˆ˜ (MIL Patch Drop)
    # ------------------------------------------------------------------
    def _random_patch_subsample(self, feats: torch.Tensor) -> torch.Tensor:
        if feats.dim() != 2 or feats.size(0) <= 1:
            return feats

        drop_min = max(0.0, min(1.0, self.mil_patch_drop_min))
        drop_max = max(0.0, min(1.0, self.mil_patch_drop_max))
        if drop_max <= 0.0:
            return feats
        if drop_min > drop_max:
            drop_min, drop_max = drop_max, drop_min

        drop_ratio = random.uniform(drop_min, drop_max)
        keep_ratio = 1.0 - drop_ratio
        num_keep = max(1, int(round(keep_ratio * feats.size(0))))

        if num_keep >= feats.size(0):
            return feats

        idx = torch.randperm(feats.size(0), device=feats.device)[:num_keep]
        return feats[idx]

    # ------------------------------------------------------------------
    # forward
    # ------------------------------------------------------------------
    def forward(self, feats, label=None, train=False, get_attention=False):
        """
        ë°˜í™˜ê°’:
          - train/val/test ê³µí†µ: loss0, loss1, gSlideLogits
          - get_attention=True: ì¶”ê°€ë¡œ tAA, patch_indices
        """
        # ---------- 1st tier ----------
        if get_attention:
            loss0, slide_pseudo_feat, tAA, patch_indices = dtfdmil_forward_1st_tier(
                self.args,
                feats,
                self.classifier,
                self.attention,
                self.dimReduction,
                self.loss_func0,
                label,
                get_attention=True,
            )
        else:
            loss0, slide_pseudo_feat = dtfdmil_forward_1st_tier(
                self.args,
                feats,
                self.classifier,
                self.attention,
                self.dimReduction,
                self.loss_func0,
                label,
            )

        if train:
            # Manual Backward (Tier 1)
            self.manual_backward(loss0, retain_graph=True)
            torch.nn.utils.clip_grad_norm_(self.dimReduction.parameters(), self.args.grad_clipping)
            torch.nn.utils.clip_grad_norm_(self.attention.parameters(), self.args.grad_clipping)
            torch.nn.utils.clip_grad_norm_(self.classifier.parameters(), self.args.grad_clipping)

        # ---------- 2nd tier ----------
        loss1, gSlideLogits = dtfdmil_forward_2nd_tier(
            slide_pseudo_feat, self.UClassifier, self.loss_func1, label
        )

        if train:
            # Manual Backward (Tier 2)
            self.manual_backward(loss1)
            torch.nn.utils.clip_grad_norm_(self.UClassifier.parameters(), self.args.grad_clipping)

        # â— ì—¬ê¸°ì„œëŠ” softmax í•˜ì§€ ì•ŠìŒ (logits ê·¸ëŒ€ë¡œ ì‚¬ìš©)
        if get_attention:
            return loss0, loss1, gSlideLogits, tAA, patch_indices
        return loss0, loss1, gSlideLogits

    # ------------------------------------------------------------------
    # training_step
    # ------------------------------------------------------------------
    def training_step(self, batch, batch_idx):
        name, coords, feats, label = batch
        feats = feats.squeeze(0)  # [1, N, D] -> [N, D]

        if self._use_patch_drop:
            feats = self._random_patch_subsample(feats)

        # Label Shape ì²˜ë¦¬ (ì˜ˆ: [1, 1, C] -> [1, C])
        if label.ndim == 3:
            label = label.squeeze(0)

        # Forward & Backward (Manual Optimization)
        if isinstance(label, torch.Tensor) and label.ndim > 1:
            # Soft label handling
            loss0, loss1, Y_logits = self.forward(feats, label, train=True)
            label_idx = torch.argmax(label, dim=1)
        else:
            # Standard Hard label
            loss0, loss1, Y_logits = self.forward(feats, label.long(), train=True)
            label_idx = label.long()

        optimizer0, optimizer1 = self.optimizers()
        if (batch_idx + 1) % self.accumulate_grad_batches == 0:
            optimizer0.step()
            optimizer1.step()
            optimizer0.zero_grad()
            optimizer1.zero_grad()

        total_loss = loss0 + loss1

        # ğŸ”¥ metricsëŠ” ê·¸ë˜í”„ ë¶„ë¦¬í•´ì„œ ì—…ë°ì´íŠ¸ (ë©”ëª¨ë¦¬/ì†ë„ ì´ë“)
        self.train_metrics.update(Y_logits.detach(), label_idx)

        # ğŸ”¥ Progress barìš© (loggerì—ëŠ” ê¸°ë¡ ì•ˆ í•¨ â†’ I/O ê°ì†Œ)
        self.log(
            "train_loss",
            total_loss,
            on_step=True,
            on_epoch=False,
            sync_dist=False,
            batch_size=self.args.batch_size,
            prog_bar=True,
            logger=False,
        )
        # ğŸ”¥ ë‚´ë¶€ ê¸°ë¡ìš©: epoch í‰ê· ë§Œ loggerì— ê¸°ë¡ (MILTrainerì™€ ë™ì¼ íŒ¨í„´)
        self.log(
            "Loss/train",
            total_loss,
            on_step=False,
            on_epoch=True,
            sync_dist=False,
            batch_size=self.args.batch_size,
            prog_bar=False,
            logger=True,
        )

        return total_loss

    def on_train_epoch_end(self):
        # Schedulers Step (optimizer0/1ì— ëŒ€í•´ CosineAnnealingLR ë™ì‘)
        sch0, sch1 = self.lr_schedulers()
        sch0.step()
        sch1.step()

        # Metrics Logging
        self.log_dict(self.train_metrics.compute(), sync_dist=False)
        self.train_metrics.reset()

    # ------------------------------------------------------------------
    # validation_step
    # ------------------------------------------------------------------
    def validation_step(self, batch, batch_idx):
        name, coords, feats, label = batch
        feats = feats.squeeze(0)

        # Label Shape ì²˜ë¦¬
        if label.ndim == 3:
            label = label.squeeze(0)

        if isinstance(label, torch.Tensor) and label.ndim > 1:
            loss0, loss1, Y_logits = self.forward(feats, label, train=False)
            label_idx = torch.argmax(label, dim=1)
        else:
            loss0, loss1, Y_logits = self.forward(feats, label.long(), train=False)
            label_idx = label.long()

        total_loss = loss0 + loss1

        # ğŸ”¥ detach í›„ metric ì—…ë°ì´íŠ¸
        self.val_metrics.update(Y_logits.detach(), label_idx)

        # Progress Barì— í‘œì‹œ (val_loss)
        self.log(
            "val_loss",
            total_loss,
            on_step=False,
            on_epoch=True,
            sync_dist=False,
            batch_size=self.args.batch_size,
            prog_bar=True,
        )
        # Checkpointìš©
        self.log(
            "Loss/val",
            total_loss,
            on_step=False,
            on_epoch=True,
            sync_dist=False,
            batch_size=self.args.batch_size,
            prog_bar=False,
        )

    def on_validation_epoch_end(self):
        metrics = self.val_metrics.compute()
        bal_acc = metrics["val/balanced_acc"].item() * 100.0

        print(f"[VAL][seed={self.seed}] Balanced Accuracy (macro): {bal_acc:.2f}%")

        # Monitorìš© ì´ë¦„ (ACC_balanced/val) ë° Progress Barìš© ì´ë¦„ (val_bacc)
        self.log("ACC_balanced/val", bal_acc, sync_dist=False)
        self.log("val_bacc", bal_acc, prog_bar=True, sync_dist=False)

        # Weighted Sampler ì‚¬ìš© ì‹œ Loss ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ëŒ€ì²´ ê°’ (Loss/val_monitor)
        if getattr(self.args, "use_weighted_sampler", False):
            self.log("Loss/val_monitor", 100.0 - bal_acc, sync_dist=False)

        self.log_dict(metrics, sync_dist=False)
        self.val_metrics.reset()

    # ------------------------------------------------------------------
    # Test Hooks & Step
    # ------------------------------------------------------------------
    def on_test_epoch_start(self):
        # seed / ë°ì´í„°ì…‹ë§ˆë‹¤ ì´ˆê¸°í™”
        self.test_outputs = []
        self.y_prob_list = []
        self.label_list = []
        self.names = []
        self.logits = None
        self.labels = None

    def test_step(self, batch, batch_idx):
        name, coords, feats, label = batch
        feats = feats.squeeze(0)

        if label.ndim == 3:
            label = label.squeeze(0)

        # DTFDëŠ” Attention Mapì„ ì–»ìœ¼ë ¤ë©´ tAAì™€ patch_indicesê°€ í•„ìš”í•¨
        attn_weights = None
        target_label = label if label.ndim > 1 else label.long()

        if self.args.attention:
            loss0, loss1, Y_logits, tAA, patch_indices = self.forward(
                feats, label=target_label, train=False, get_attention=True
            )
            # DTFD: tAAëŠ” pseudo bagì˜ attention score
            attn_weights = tAA.detach().cpu()
        else:
            loss0, loss1, Y_logits = self.forward(feats, label=target_label, train=False)

        # ğŸ”¥ 1) ê·¸ë£¹ ì°¨ì›ì´ ìˆì„ ê²½ìš° â†’ ìŠ¬ë¼ì´ë“œ ë‹¨ìœ„ë¡œ ì§‘ê³„
        #  - Y_logits: (G, C) ì¸ ê²½ìš°ê°€ ìˆìŒ (G = numGroup)
        if Y_logits.dim() == 1:
            # (C,)ì¸ ê²½ìš° â†’ (1, C)
            Y_logits_slide = Y_logits.unsqueeze(0)
        elif Y_logits.dim() == 2 and Y_logits.size(0) > 1:
            # (G, C)ì¸ ê²½ìš° â†’ ê·¸ë£¹ í‰ê· ìœ¼ë¡œ ìŠ¬ë¼ì´ë“œ ëŒ€í‘œ logit ìƒì„±
            Y_logits_slide = Y_logits.mean(dim=0, keepdim=True)
        else:
            # ì´ë¯¸ (1, C)í˜•íƒœë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            Y_logits_slide = Y_logits

        # ğŸ”¥ 2) testì—ì„œëŠ” calibration ìœ„í•´ softmax í•œ ë²ˆë§Œ ì ìš© (í™•ë¥  ì €ì¥)
        Y_prob = F.softmax(Y_logits_slide, dim=1)

        # ë¼ë²¨ ì¸ë±ìŠ¤ (soft/hard label ëª¨ë‘ ì²˜ë¦¬) â†’ 1D í…ì„œë¡œ ë§ì¶”ê¸°
        if label.ndim > 1:
            label_idx = torch.argmax(label, dim=1)
        else:
            label_idx = label.view(-1)  # scalar â†’ (1,)

        total_loss = loss0 + loss1

        # ğŸ”¥ 3) metricsë„ ìŠ¬ë¼ì´ë“œ ë‹¨ìœ„ í™•ë¥ ë¡œ ì—…ë°ì´íŠ¸ (detach)
        self.test_metrics.update(Y_prob.detach(), label_idx)
        self.log("Loss/final_test", total_loss, on_epoch=True, sync_dist=False)

        # ğŸ”¥ 4) Callback ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì €ì¥ (CPU ì´ë™)
        slide_name = name[0] if isinstance(name, (list, tuple)) else name

        self.test_outputs.append(
            {
                "name": slide_name,
                "coords": coords,
                "probs": Y_prob.detach().cpu(),     # (1, C) ìŠ¬ë¼ì´ë“œ ë‹¨ìœ„
                "label": label_idx.detach().cpu(),  # (1,)
                "attn": attn_weights,               # DTFDì˜ ê²½ìš° tAA (Tier-1 attention scores)
            }
        )

        # ğŸ”¥ 5) save_metrics.py í˜¸í™˜ìš© ë²„í¼ë„ ë™ì‹œì— ì±„ìš°ê¸°
        self.y_prob_list.append(Y_prob.detach().cpu())    # (1, C)
        self.label_list.append(label_idx.detach().cpu())  # (1,)
        self.names.append(slide_name)

    def on_test_epoch_end(self):
        # 1) ë©”íŠ¸ë¦­ ì§‘ê³„
        self.log_dict(self.test_metrics.compute(), sync_dist=False)
        self.test_metrics.reset()

        # 2) save_metrics.py í˜¸í™˜ìš© ì†ì„± ì±„ìš°ê¸°
        if len(self.y_prob_list) > 0:
            # (N, C) í™•ë¥  í…ì„œ
            self.logits = torch.cat(self.y_prob_list, dim=0).detach().cpu()
        else:
            self.logits = None

        if len(self.label_list) > 0:
            # (N,) ë¼ë²¨ í…ì„œ
            self.labels = torch.cat(self.label_list, dim=0).detach().cpu()
        else:
            self.labels = None
        # self.namesëŠ” ë¦¬ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        # self.test_outputsëŠ” callbacks.analysis_callbackì—ì„œ ì‚¬ìš© í›„ ë¹„ì›Œì§

    # ------------------------------------------------------------------
    # Optimizer / Scheduler
    # ------------------------------------------------------------------
    def configure_optimizers(self):
        # ---- 1. íŒŒë¼ë¯¸í„° ê·¸ë£¹í•‘ ----
        trainable_parameters = []
        trainable_parameters += list(self.classifier.parameters())
        trainable_parameters += list(self.attention.parameters())
        trainable_parameters += list(self.dimReduction.parameters())

        params_optimizer_0 = [{"params": filter(lambda p: p.requires_grad, trainable_parameters)}]
        params_optimizer_1 = [{"params": filter(lambda p: p.requires_grad, self.UClassifier.parameters())}]

        # ---- 2. ì˜µí‹°ë§ˆì´ì € ìƒì„± í—¬í¼ ----
        def create_optimizer(params, opt_name, lr, wd):
            opt_name = opt_name.lower()
            if opt_name == "adam":
                return torch.optim.Adam(params, lr=lr, weight_decay=wd)
            elif opt_name == "adamw":
                return torch.optim.AdamW(params, lr=lr, weight_decay=wd)
            elif opt_name == "radam":
                return torch.optim.RAdam(params, lr=lr, weight_decay=wd)
            elif opt_name == "lookahead_radam":
                base = torch.optim.RAdam(params, lr=lr, weight_decay=wd)
                return Lookahead(base, k=5, alpha=0.5)
            elif opt_name == "lion":
                return Lion(params, lr=lr, betas=(0.9, 0.99), weight_decay=wd)
            else:
                return torch.optim.Adam(params, lr=lr, weight_decay=wd)

        opt_name = self.args.opt or "adam"
        optimizer0 = create_optimizer(
            params_optimizer_0, opt_name, self.args.lr, self.args.weight_decay
        )
        optimizer1 = create_optimizer(
            params_optimizer_1, opt_name, self.args.lr, self.args.weight_decay
        )

        # ---- 3. ìŠ¤ì¼€ì¤„ëŸ¬ ----
        scheduler0 = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer0, T_max=self.args.epochs, eta_min=5e-6
        )
        scheduler1 = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer1, T_max=self.args.epochs, eta_min=5e-6
        )

        return [optimizer0, optimizer1], [scheduler0, scheduler1]
